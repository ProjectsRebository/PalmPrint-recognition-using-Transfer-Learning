{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel('features_vgg16_Tongi_session1_right_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6',\n",
       "       'feature7', 'feature8', 'feature9', 'feature10',\n",
       "       ...\n",
       "       'feature4088', 'feature4089', 'feature4090', 'feature4091',\n",
       "       'feature4092', 'feature4093', 'feature4094', 'feature4095',\n",
       "       'feature4096', 'class'],\n",
       "      dtype='object', length=4097)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['class'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data= data1 # 310 \n",
    "\n",
    "data_persons = len(data['class'].unique())\n",
    "train_size,validate_size,test_size=int(data_persons*0.7), int(data_persons*.15), int(data_persons*.15)\n",
    "train_data= data[data['class'].isin( list(range(1,train_size) ))]\n",
    "validate_data= data[data['class'].isin( list(range(train_size,train_size+validate_size ) ))]\n",
    "test_data= data[data['class'].isin( list(range(train_size+validate_size ,train_size+validate_size +test_size ) ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2090, 4097), (450, 4097), (450, 4097))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, validate_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature4087', 'feature4088', 'feature4089', 'feature4090', 'feature4091', 'feature4092', 'feature4093', 'feature4094', 'feature4095', 'feature4096']\n"
     ]
    }
   ],
   "source": [
    "features = list(data.columns)\n",
    "features.remove('class')\n",
    "print(features[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2090, 4096), (450, 4096))"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data[features]\n",
    "y_train = train_data['class']\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data['class']\n",
    "\n",
    "\n",
    "X_validate = validate_data[features]\n",
    "y_validate = validate_data['class']\n",
    "\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.17978223, 0.        , ..., 0.        , 1.20329253,\n",
       "       0.73604059])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def getMean(df, list_index, n):\n",
    "    #print( list_index,n)\n",
    "    selected_list_index = np.random.choice(list_index,n,replace=False)\n",
    "    return df.iloc[selected_list_index].mean()\n",
    "    \n",
    "\n",
    "getMean(X_train, [1,2,3,4,5], 3).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "\n",
    "def make_pairs(images, labels,No_images_avarage=1):\n",
    "\t# initialize two empty lists to hold the (image, image) pairs and\n",
    "\t# labels to indicate if a pair is positive or negative\n",
    "\tpairImages = []\n",
    "\tpairLabels = []\n",
    "\tpairPersons= []\n",
    "\timages.reset_index(inplace=True, drop=True) \n",
    "\tlabels.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "\t# calculate the total number of classes present in the dataset\n",
    "\t# and then build a list of indexes for each class label that\n",
    "\t# provides the indexes for all examples with a given label\n",
    "\t# loop over all images\n",
    "\tcounter = 0\n",
    "\ttrain_labels_pd = pd.DataFrame({'label': labels})\n",
    "\tparent_list=[]\n",
    "\tfor idxA in range(len(images)):\n",
    "\t\t# grab the current image and label belonging to the current\n",
    "\t\t# iteration\n",
    "\t\tcurrentImage = images.iloc[idxA]\n",
    "\t\t#print(idxA)\n",
    "\t\tlabel = labels.iloc[idxA]\n",
    "        #get indeces of images of same label \n",
    "\t\tparent_list.append(idxA)\n",
    "\t\tindexes_same_label =list( train_labels_pd[train_labels_pd.label==label].index)\n",
    "\t\tindexes_same_label = list(set(indexes_same_label)-set(parent_list)) \n",
    "\t\tindexes_diffrent_label =list( train_labels_pd[train_labels_pd.label!=label].index)\n",
    "\n",
    "\t\tfor idxB in indexes_same_label: #loop over same label one by one\n",
    "\t\t\tif idxB !=idxA:\n",
    "\t\t\t\t#posImage = images.iloc[idxB]\n",
    "\t\t\t\t# this condition will prevent error when indexes_same_label become less than 3( p.random.choice gives error)\n",
    "\t\t\t\tif len(indexes_same_label)< No_images_avarage :  \n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tavarage_Image = getMean(images, indexes_same_label, No_images_avarage).values #use avarage of same class data  \n",
    "\t\t\t\tposImage = avarage_Image #use avarage of same class data  \n",
    "                # prepare a positive pair and update the images and labels\n",
    "                # lists, respectively\n",
    "\t\t\t\tpairImages.append([currentImage, posImage])\n",
    "\t\t\t\tpairLabels.append([1])\n",
    "\t\t\t\tpairPersons.append([ labels.iloc[idxA],  labels.iloc[idxA]])\n",
    "\n",
    "                \n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\tif (counter %1000)==0:\n",
    "\t\t\t\t\tprint(counter)\n",
    "                \n",
    "                # grab the indices for each of the class labels *not* equal to\n",
    "                # the current label and randomly pick an image corresponding\n",
    "                # to a label *not* equal to the current label\n",
    "\t\t\t\tnegIdx = np.random.choice(indexes_diffrent_label)\n",
    "\t\t\t\t#print(idxA,idxB,negIdx)                \n",
    "\n",
    "                #negIdx = np.where(labels != label)[0]\n",
    "\t\t\t\tnegImage = images.iloc[negIdx]\n",
    "                #print('Neg', idxA,negIdx)\n",
    "\n",
    "                # prepare a negative pair of images and update our lists\n",
    "                \n",
    "\t\t\t\t#pairImages.append([currentImage, negImage])\n",
    "\t\t\t\tpairImages.append([avarage_Image, negImage]) # use avarage image instead of image\n",
    "                \n",
    "\t\t\t\tpairLabels.append([0])\n",
    "\t\t\t\tpairPersons.append([ labels.iloc[idxA],  labels.iloc[negIdx]])\n",
    "                \n",
    "                \n",
    "\n",
    "\t# return a 2-tuple of our image pairs and labels\n",
    "\treturn np.array(pairImages), np.array(pairLabels), np.array(pairPersons)\n",
    "\n",
    "#return \n",
    "#0 1\n",
    "#0 2\n",
    "#0 8\n",
    "#1 2\n",
    "#1 3\n",
    "#1 8\n",
    "#2 3 \n",
    "#.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "\n",
    "data_folder= 'binary_data/'\n",
    "def store_data(name, dat):\n",
    "    savez_compressed(data_folder +name+'.npz', dat)\n",
    "    \n",
    "def load_data(name):\n",
    "    data_loaded = load(data_folder +name+'.npz')\n",
    "    return data_loaded['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store dataset \n",
    "def store_all_data( pairX,Label,pairPersons,datasetName='DS1', no_images_avg=2,da_type='Train' ):\n",
    "    no_images_avg = str(no_images_avg)\n",
    "    store_data(datasetName+ '_avg'+ no_images_avg +'_pair'+da_type+'X',pairX)\n",
    "    store_data(datasetName+ '_avg'+ no_images_avg +'_Label'+da_type,Label)\n",
    "    store_data(datasetName+ '_avg'+ no_images_avg +'_pairPersons'+da_type,pairPersons)\n",
    "\n",
    "def load_all_data( datasetName='DS1', no_images_avg=2,da_type='Train' ):\n",
    "    no_images_avg = str(no_images_avg)\n",
    "    pairX = load_data(datasetName+ '_avg'+ no_images_avg +'_pair'+da_type+'X')\n",
    "    Label = load_data(datasetName+ '_avg'+ no_images_avg +'_Label'+da_type)\n",
    "    pairPersons = load_data(datasetName+ '_avg'+ no_images_avg +'_pairPersons'+da_type)\n",
    "    return pairX,Label,pairPersons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasetName='Diff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((18810, 2, 4096), (18810, 1), (18810, 2))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairTrainX, LabelTrain,pairPersonsTrain = make_pairs(X_train, y_train,no_images_avg)\n",
    "pairTrainX.shape , LabelTrain.shape, pairPersonsTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4050, 2, 4096), (4050, 1), (4050, 2))"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.reset_index(inplace=True, drop=True) \n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "pairTestX, labelTest,pairPersonsTest = make_pairs(X_test, y_test,no_images_avg)\n",
    "pairTestX.shape, labelTest.shape,pairPersonsTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4050, 2, 4096), (4050, 1), (4050, 2))"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validate.reset_index(inplace=True, drop=True) \n",
    "y_validate.reset_index(inplace=True, drop=True)\n",
    "pairValX, labelVal ,pairPersonsVal= make_pairs(X_validate, y_validate,no_images_avg)\n",
    "pairValX.shape , labelVal.shape, pairPersonsVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nstore_all_data( pairTrainX, LabelTrain, pairPersonsTrain,datasetName=datasetName, no_images_avg=no_images_avg,da_type='Train' )\\nstore_all_data( pairValX, labelVal ,pairPersonsVal,datasetName=datasetName, no_images_avg=no_images_avg,da_type='Val' )\\nstore_all_data( pairTestX, labelTest,pairPersonsTest,datasetName=datasetName, no_images_avg=no_images_avg,da_type='Test' )\\n\""
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "store_all_data( pairTrainX, LabelTrain, pairPersonsTrain,datasetName=datasetName, no_images_avg=no_images_avg,da_type='Train' )\n",
    "store_all_data( pairValX, labelVal ,pairPersonsVal,datasetName=datasetName, no_images_avg=no_images_avg,da_type='Val' )\n",
    "store_all_data( pairTestX, labelTest,pairPersonsTest,datasetName=datasetName, no_images_avg=no_images_avg,da_type='Test' )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npairTrainX, LabelTrain, pairPersonsTrain =load_all_data(datasetName='DS2', no_images_avg=no_images_avg,da_type='Train' )\\npairValX, labelVal ,pairPersonsVal = load_all_data( datasetName='DS2', no_images_avg=no_images_avg,da_type='Val' )\\npairTestX, labelTest,pairPersonsTest= load_all_data( datasetName='DS2', no_images_avg=no_images_avg,da_type='Test' )\\n\""
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pairTrainX, LabelTrain, pairPersonsTrain =load_all_data(datasetName='DS2', no_images_avg=no_images_avg,da_type='Train' )\n",
    "pairValX, labelVal ,pairPersonsVal = load_all_data( datasetName='DS2', no_images_avg=no_images_avg,da_type='Val' )\n",
    "pairTestX, labelTest,pairPersonsTest= load_all_data( datasetName='DS2', no_images_avg=no_images_avg,da_type='Test' )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[101, 101],\n",
       "        [101,  73],\n",
       "        [101, 101],\n",
       "        [101,  53],\n",
       "        [101, 101],\n",
       "        [101, 183],\n",
       "        [101, 101],\n",
       "        [101, 131],\n",
       "        [101, 101],\n",
       "        [101,  73]], dtype=int64),\n",
       " array([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairPersonsTrain[100:110],LabelTrain[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 4096)         0           input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4096)         16384       lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            4097        batch_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 12,289\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,concatenate,Dropout,BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "input_shape = [4096]\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "\n",
    "# Add a customized layer to compute the absolute difference between the encodings\n",
    "#L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#L1_distance = L1_layer([left_input, right_input])\n",
    "\n",
    "# Add a customized layer to compute the ecludian distance\n",
    "L2_layer = Lambda(lambda tensors:K.sqrt(K.square( K.abs(tensors[0] - tensors[1]) )))\n",
    "L2_distance = L2_layer([left_input, right_input])\n",
    "\n",
    "dens = Dense(256,activation='sigmoid')(L2_distance)\n",
    "#drop = Dropout(50)(dens)\n",
    "#bn = BatchNormalization()(dens)\n",
    "bn = BatchNormalization()(L2_distance)\n",
    "\n",
    "# Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "#enable next line and disable after next to train without extra dens layer   \n",
    "#prediction = Dense(1,activation='sigmoid',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(L2_distance)\n",
    "prediction = Dense(1,activation='sigmoid')(bn)#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(bn)\n",
    "\n",
    "# Connect the inputs with the outputs\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "optimizer = Adam()\n",
    "\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",metrics=['accuracy'], optimizer=optimizer)\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#siamese_net.count_params()\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18810, 4096)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairTrainX1=pairTrainX[:,0,:]\n",
    "pairTrainX2= pairTrainX[:,1,:]\n",
    "\n",
    "pairTestX1= pairTestX[:,0,:]\n",
    "pairTestX2= pairTestX[:,1,:]\n",
    "\n",
    "pairValX1= pairValX[:,0,:]\n",
    "pairValX2= pairValX[:,1,:]\n",
    "\n",
    "pairTrainX1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4050, 4096)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairValX1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "63/63 [==============================] - 10s 136ms/step - loss: 0.2291 - accuracy: 0.9007 - val_loss: 0.4581 - val_accuracy: 0.7420\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.74198, saving model to best_model.h5\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 8s 126ms/step - loss: 0.0647 - accuracy: 0.9773 - val_loss: 0.3579 - val_accuracy: 0.8894\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.74198 to 0.88938, saving model to best_model.h5\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 9s 140ms/step - loss: 0.0386 - accuracy: 0.9883 - val_loss: 0.2714 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.88938 to 0.91407, saving model to best_model.h5\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 0.1789 - val_accuracy: 0.9543\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.91407 to 0.95432, saving model to best_model.h5\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 0.0207 - accuracy: 0.9958 - val_loss: 0.1160 - val_accuracy: 0.9731\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.95432 to 0.97309, saving model to best_model.h5\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 0.0169 - accuracy: 0.9966 - val_loss: 0.0844 - val_accuracy: 0.9751\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.97309 to 0.97506, saving model to best_model.h5\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 0.0151 - accuracy: 0.9975 - val_loss: 0.0675 - val_accuracy: 0.9758\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.97506 to 0.97580, saving model to best_model.h5\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0117 - accuracy: 0.9981 - val_loss: 0.0622 - val_accuracy: 0.9770\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.97580 to 0.97704, saving model to best_model.h5\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0114 - accuracy: 0.9981 - val_loss: 0.0589 - val_accuracy: 0.9765\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.97704\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 8s 119ms/step - loss: 0.0085 - accuracy: 0.9990 - val_loss: 0.0570 - val_accuracy: 0.9765\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.97704\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 0.0076 - accuracy: 0.9994 - val_loss: 0.0567 - val_accuracy: 0.9780\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.97704 to 0.97802, saving model to best_model.h5\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 9s 137ms/step - loss: 0.0064 - accuracy: 0.9993 - val_loss: 0.0553 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.97802 to 0.97951, saving model to best_model.h5\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 0.0056 - accuracy: 0.9998 - val_loss: 0.0554 - val_accuracy: 0.9793\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.97951\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 0.0548 - val_accuracy: 0.9783\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.97951\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 9s 136ms/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 0.0552 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.97951\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 7s 119ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.97951 to 0.98049, saving model to best_model.h5\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.0558 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.98049\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.0548 - val_accuracy: 0.9788\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.98049\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0031 - accuracy: 0.9999 - val_loss: 0.0551 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.98049\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.98049\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 8s 126ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.98049\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.98049\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.98049\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 8s 124ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0592 - val_accuracy: 0.9793\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.98049\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.98049\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.98049\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.98049\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.98049\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 9s 138ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.98049\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 8s 124ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.98049\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.98049\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 8s 124ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.98049\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.98049\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.98049\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 8.6665e-04 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.98049\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 7.9696e-04 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.98049\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 8.5895e-04 - accuracy: 1.0000 - val_loss: 0.0623 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.98049\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 9s 136ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9807\n",
      "\n",
      "Epoch 00038: val_accuracy improved from 0.98049 to 0.98074, saving model to best_model.h5\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 7.9376e-04 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.98074\n",
      "Epoch 40/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 114ms/step - loss: 6.2268e-04 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9807\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.98074\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 8s 124ms/step - loss: 7.1289e-04 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.98074\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 6.0303e-04 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.98074\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 7s 110ms/step - loss: 6.4514e-04 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.98074\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 7.1478e-04 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.98074\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 7s 111ms/step - loss: 6.9508e-04 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.98074\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 8.3625e-04 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.98074\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 8s 132ms/step - loss: 7.3143e-04 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9807\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.98074\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 7.0640e-04 - accuracy: 1.0000 - val_loss: 0.0672 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.98074\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 5.2928e-04 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.98074\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 5.7216e-04 - accuracy: 1.0000 - val_loss: 0.0659 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.98074\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 4.8904e-04 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9810\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.98074 to 0.98099, saving model to best_model.h5\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 7s 111ms/step - loss: 3.7528e-04 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.98099\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 3.6634e-04 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.98099\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 3.4387e-04 - accuracy: 1.0000 - val_loss: 0.0687 - val_accuracy: 0.9807\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.98099\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 3.0179e-04 - accuracy: 1.0000 - val_loss: 0.0701 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.98099\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 9s 148ms/step - loss: 4.4612e-04 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.98099\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 7s 111ms/step - loss: 3.3597e-04 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.98099\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 2.8788e-04 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.98099\n",
      "Epoch 59/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 4.1484e-04 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00059: val_accuracy improved from 0.98099 to 0.98148, saving model to best_model.h5\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 4.1639e-04 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.98148\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 8s 123ms/step - loss: 4.2573e-04 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.98148\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 8s 120ms/step - loss: 3.4631e-04 - accuracy: 1.0000 - val_loss: 0.0701 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.98148\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 8s 120ms/step - loss: 2.1522e-04 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.98148\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 3.3577e-04 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.98148\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 9s 148ms/step - loss: 3.0427e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.98148\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 1.9442e-04 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.98148\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 2.3961e-04 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.98148\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 8s 125ms/step - loss: 2.2534e-04 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.98148\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 1.9304e-04 - accuracy: 1.0000 - val_loss: 0.0772 - val_accuracy: 0.9793\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.98148\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 2.7364e-04 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.98148\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 1.6015e-04 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.98148\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 1.6667e-04 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.98148\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 8s 123ms/step - loss: 2.0935e-04 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.98148\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 2.1131e-04 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.98148\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 1.6153e-04 - accuracy: 1.0000 - val_loss: 0.0783 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.98148\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 1.4272e-04 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9793\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.98148\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 2.2803e-04 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.98148\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 1.2387e-04 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.98148\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 1.6143e-04 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.98148\n",
      "Epoch 80/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 117ms/step - loss: 1.6438e-04 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.98148\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 8s 126ms/step - loss: 2.0945e-04 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.98148\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 9s 142ms/step - loss: 1.6783e-04 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.98148\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 1.5623e-04 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.98148\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 1.1799e-04 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.98148\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 1.4567e-04 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9802\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.98148\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 1.2083e-04 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.98148\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 1.0304e-04 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.98148\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 7s 119ms/step - loss: 1.1406e-04 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.98148\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 9.8354e-05 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.98148\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 1.2664e-04 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.98148\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 8s 134ms/step - loss: 9.4759e-05 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.98148\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 8.8100e-05 - accuracy: 1.0000 - val_loss: 0.0828 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.98148\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 7s 111ms/step - loss: 7.7931e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.98148\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 8s 128ms/step - loss: 1.0608e-04 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9793\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.98148\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 1.0523e-04 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.98148\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 7s 115ms/step - loss: 1.1928e-04 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9793\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.98148\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 6.7383e-05 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.98148\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 7s 111ms/step - loss: 1.0476e-04 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.98148\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 8.6847e-05 - accuracy: 1.0000 - val_loss: 0.0844 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.98148\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 8s 126ms/step - loss: 8.0845e-05 - accuracy: 1.0000 - val_loss: 0.0853 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.98148\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 7s 113ms/step - loss: 5.7852e-05 - accuracy: 1.0000 - val_loss: 0.0831 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.98148\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 5.9332e-05 - accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9800\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.98148\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 8.9799e-05 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9807\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.98148\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 8s 120ms/step - loss: 7.5138e-05 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.98148\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 7s 116ms/step - loss: 4.9857e-05 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.98148\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 4.7277e-05 - accuracy: 1.0000 - val_loss: 0.0863 - val_accuracy: 0.9798\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.98148\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 7s 112ms/step - loss: 4.2909e-05 - accuracy: 1.0000 - val_loss: 0.0899 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.98148\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 5.7406e-05 - accuracy: 1.0000 - val_loss: 0.0864 - val_accuracy: 0.9790\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.98148\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 9s 142ms/step - loss: 4.5444e-05 - accuracy: 1.0000 - val_loss: 0.0877 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.98148\n",
      "Epoch 00109: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "cb = [EarlyStopping(monitor='val_accuracy', mode='max', verbose=2, patience=50),\n",
    "     ModelCheckpoint(filepath='best_model.h5', mode='max',monitor='val_accuracy', verbose=2,save_best_only=True)]\n",
    "\n",
    "        \n",
    "epochs=300\n",
    "import tensorflow as tf\n",
    "\n",
    "#fixes the romdon for repredcetable \n",
    "np.random.seed(7)\n",
    "#with tf.device('/gpu:0'):\n",
    "history=siamese_net.fit(\n",
    "            [pairTrainX1,pairTrainX2], LabelTrain,\n",
    "            batch_size=300,  \n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            shuffle = True,\n",
    "           validation_data=([pairValX1,pairValX2], labelVal),\n",
    "            callbacks=cb\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814814925193787"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 1s 10ms/step - loss: 0.0765 - accuracy: 0.9815 0s - loss: 0.0803 - accuracy: 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07654169946908951, 0.9814814925193787]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.load_weights('best_model.h5')\n",
    "y_pred_val = siamese_net.predict([pairValX1,pairValX2])\n",
    "\n",
    "siamese_net.evaluate([pairValX1,pairValX2], labelVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 1s 7ms/step - loss: 0.0857 - accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08568112552165985, 0.9772839546203613]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.evaluate([pairTestX1,pairTestX2], labelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9975413504039019"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tst = siamese_net.predict([pairTestX1,pairTestX2])\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,confusion_matrix\n",
    "\n",
    "roc_auc_score(labelTest, y_pred_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1993,   32],\n",
       "       [  60, 1965]], dtype=int64)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labelTest, y_pred_tst[:]>0.5)\n",
    "#incorrect 196 -- avarage=3\n",
    "#incorrect 187 -- avarage=4\n",
    "#incorrect 184 -- avarage=4 without dens layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics  import accuracy_score, auc, roc_curve, precision_recall_curve\n",
    "\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "        \n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), \n",
    "                        'threshold' : pd.Series(threshold, index=i),\n",
    "                       'tpr':pd.Series(tpr, index=i),\n",
    "                        'fpr':pd.Series(fpr, index=i),\n",
    "                        'tnr' : pd.Series((1-fpr), index=i),\n",
    "                        's' : pd.Series((1-fpr) + tpr, index=i),\n",
    "                        \n",
    "\n",
    "                        \n",
    "                       })\n",
    "    #return roc\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return roc_t['threshold'].values[0], roc_t['tpr'].values[0],roc_t['tnr'].values[0],roc_t['fpr'].values[0] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simpler implementation of the J statistic\n",
    "\n",
    "def get_optimal_threshold(y_pred, y_true):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    # calculate the the Youdens J statistic\n",
    "    J = tpr - fpr\n",
    "        \n",
    "    # locate the index of the largest g-mean\n",
    "    ix = np.argmax(J)\n",
    "    print('Best Threshold=%f' % (thresholds[ix]))\n",
    "    return thresholds[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.472069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4720686"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cutoff = get_optimal_threshold( y_pred_tst,labelTest)\n",
    "cutoff = get_optimal_threshold(y_pred_val,labelVal)\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cutoff=0.5\n",
    "accuracy_score(labelTest, y_pred_tst>cutoff )\n",
    "#accuracy_score(labelVal,y_pred_val>cutoff )\n",
    "#0.8946428571428572 - 32\n",
    "#0.9014285714285715 - 256\n",
    "#0.8928571428571429 - 64\n",
    "#0.8775 without extra Dens layer \n",
    "#0.9081481481481481 -- avarage=2\n",
    "#0.9204 -- avarage=3\n",
    "#0.922 -- avarage=4\n",
    "#0.920 -- avarage=4 without dens layer\n",
    "#0.925 -- avarage=4 without dens layer + batchnor\n",
    "#0.9224 -- avarage=3 without dens layer + batchnor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1993,   32],\n",
       "       [  58, 1967]], dtype=int64)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labelTest, y_pred_tst>cutoff)\n",
    "#array([[1364,   36],  --32\n",
    "#       [ 254, 1146]], dtype=int64)\n",
    "\n",
    "#array([[1340,   60],  -256\n",
    "#       [ 216, 1184]], dtype=int64)\n",
    "#incorrect 248 -- > avarage=2\n",
    "#incorrect 199 -- > avarage=3\n",
    "#incorrect 170 -- > avarage=4\n",
    "#incorrect 196 -- > avarage=4 without dens layer\n",
    "#incorrect 196 -- > avarage=4 without dens layer + batchnor\n",
    "#incorrect 196 -- > avarage=3 without dens layer + batchnor\n",
    "#incorrect 194 -- > avarage=4 without dens layer + batchnor\n",
    "#incorrect 227 -- > avarage=2 without dens layer + batchnor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Same_Rate(TP)</th>\n",
       "      <th>Diff_Rate(TN)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     P  Same_Rate(TP)  Diff_Rate(TN)\n",
       "0  255       0.955556       0.977778\n",
       "1  256       0.911111       1.000000\n",
       "2  257       1.000000       0.977778\n",
       "3  258       1.000000       1.000000\n",
       "4  259       1.000000       1.000000"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pd = pd.DataFrame()\n",
    "indexTrue = (labelTest == (y_pred_tst>cutoff) ) # True in case correct and False in case incorrect  \n",
    "\n",
    "test_pd['Correct'] = indexTrue[:,0]\n",
    "test_pd['P1']= pairPersonsTest[:,0]\n",
    "test_pd['P2']= pairPersonsTest[:,1]\n",
    "test_pd['Same']= (test_pd.P1 ==test_pd.P2 ) # same person or not\n",
    "test_pd.to_csv('test_cases.csv')\n",
    "\n",
    "#test_pd[test_pd.Correct==False].groupby(['P1','P2']).count().to_csv('incorrect_cases.csv')\n",
    "#test_pd.groupby(['P1','Same']).agg(['sum', 'count']).to_csv('all_cases.csv')\n",
    "\n",
    "testPivot = pd.pivot_table(test_pd, index=[\"P1\"], columns=[\"Same\"], values=[\"Correct\",\"P1\"], aggfunc=[np.sum,np.size])\n",
    "#testPivot[ [( 'sum', 'Correct', False), ( 'sum', 'Correct', True),( 'size', 'Correct', False) ,( 'size', 'Correct', True)  ]]\n",
    "\n",
    "test_result_pd= pd.DataFrame()\n",
    "\n",
    "test_result_pd['P']= testPivot.index\n",
    "test_result_pd['Same_Rate(TP)'] = list(testPivot[( 'sum', 'Correct', True)] /testPivot[( 'size', 'Correct', True)])\n",
    "test_result_pd['Diff_Rate(TN)'] = list(testPivot[( 'sum', 'Correct', False)] /testPivot[( 'size', 'Correct', False)])\n",
    "\n",
    "test_result_pd.to_csv('test_detail_results.csv')\n",
    "test_result_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.971358024691358, 0.9841975308641975, 0.9777777777777777)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_pd['Same_Rate(TP)'].mean(), test_result_pd['Diff_Rate(TN)'].mean() ,(test_result_pd['Same_Rate(TP)'].mean()+ test_result_pd['Diff_Rate(TN)'].mean() )/2 \n",
    "#('Same_Rate(TP) , Diff_Rate(TN) , Avarage Accurecy \n",
    "#(0.8457142857142855, 0.9571428571428575) - 256\n",
    "#(0.8199999999999998, 0.965714285714286) - 512\n",
    "#(0.8349999999999997, 0.92, 0.8775) without extra Dens layer \n",
    "#(0.8422222222222223, 0.9740740740740738, 0.908148148148148) --> avarage =2\n",
    "#(0.8543999999999999, 0.9864000000000003, 0.9204000000000001) --> avarage =3\n",
    "#(0.8772727272727272, 0.9681818181818178, 0.9227272727272725) --> avarage =4\n",
    "#(0.8809090909090911, 0.9409090909090908, 0.9109090909090909) --> avarage =4 without dens layer\n",
    "#(0.8736363636363638, 0.978181818181818, 0.9259090909090909) --> avarage Pics =4 without dens layer +BN\n",
    "#(0.8632000000000002, 0.9816000000000003, 0.9224000000000002)--> avarage Pics=3 without dens layer +BN\n",
    "#(0.8874074074074074, 0.9444444444444443, 0.9159259259259258)--> avarage Pics=2 without dens layer +BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Same_Rate(TP)</th>\n",
       "      <th>Diff_Rate(TN)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>267</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>289</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>298</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>277</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>288</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>297</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>272</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>271</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>268</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      P  Same_Rate(TP)  Diff_Rate(TN)\n",
       "12  267       0.622222       0.955556\n",
       "34  289       0.777778       0.977778\n",
       "43  298       0.866667       0.977778\n",
       "22  277       0.911111       1.000000\n",
       "1   256       0.911111       1.000000\n",
       "33  288       0.933333       1.000000\n",
       "42  297       0.955556       1.000000\n",
       "17  272       0.955556       1.000000\n",
       "16  271       0.955556       1.000000\n",
       "13  268       0.955556       1.000000"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_pd.sort_values('Same_Rate(TP)').head(10) # 7 -->avg-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Same_Rate(TP)</th>\n",
       "      <th>Diff_Rate(TN)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>267</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>298</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      P  Same_Rate(TP)  Diff_Rate(TN)\n",
       "27  282       1.000000       0.800000\n",
       "23  278       1.000000       0.911111\n",
       "11  266       1.000000       0.933333\n",
       "12  267       0.622222       0.955556\n",
       "30  285       1.000000       0.955556\n",
       "35  290       1.000000       0.955556\n",
       "25  280       1.000000       0.955556\n",
       "6   261       1.000000       0.955556\n",
       "0   255       0.955556       0.977778\n",
       "43  298       0.866667       0.977778"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_pd.sort_values('Diff_Rate(TN)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import xlsxwriter\n",
    "def compute_eer(y_pred, y_true):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    #FAR means fpr \n",
    "    #FRR means fnr\n",
    "    #GAR means tpr\n",
    "    fnr = 1-tpr\n",
    "    plt.plot(fnr)\n",
    "    plt.plot(fpr)\n",
    "    plt.show()\n",
    "    print(\"FAR\")\n",
    "    print(fpr)\n",
    "    with xlsxwriter.Workbook('FAR.xlsx') as workbook:\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        worksheet.write_row(0, 0, fpr)\n",
    "    \n",
    "    print(\"GAR\")\n",
    "    with xlsxwriter.Workbook('GAR.xlsx') as workbook:\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        worksheet.write_row(0, 0, tpr)\n",
    "    print(tpr)\n",
    "    abs_diffs = np.abs(fpr - fnr)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((fpr[min_index], fnr[min_index]))\n",
    "    # plot GAR VS FAR\n",
    "    #plt.plot(fpr)\n",
    "    #plt.plot(tpr)\n",
    "    #plt.show()\n",
    "    return eer, thresholds[min_index], fpr[min_index], fnr[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAR\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 4.93827160e-04 4.93827160e-04 4.93827160e-04\n",
      " 4.93827160e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 9.87654321e-04\n",
      " 9.87654321e-04 9.87654321e-04 9.87654321e-04 1.48148148e-03\n",
      " 1.48148148e-03 1.48148148e-03 1.48148148e-03 1.48148148e-03\n",
      " 1.48148148e-03 1.48148148e-03 1.48148148e-03 1.48148148e-03\n",
      " 1.48148148e-03 1.48148148e-03 1.48148148e-03 1.48148148e-03\n",
      " 1.48148148e-03 1.48148148e-03 1.48148148e-03 1.48148148e-03\n",
      " 1.48148148e-03 1.48148148e-03 1.48148148e-03 1.48148148e-03\n",
      " 1.48148148e-03 1.48148148e-03 2.96296296e-03 2.96296296e-03\n",
      " 3.45679012e-03 3.45679012e-03 3.45679012e-03 3.45679012e-03\n",
      " 3.45679012e-03 3.45679012e-03 3.45679012e-03 3.45679012e-03\n",
      " 3.45679012e-03 3.45679012e-03 3.45679012e-03 3.45679012e-03\n",
      " 4.44444444e-03 4.44444444e-03 4.93827160e-03 4.93827160e-03\n",
      " 5.43209877e-03 5.43209877e-03 5.43209877e-03 5.43209877e-03\n",
      " 5.92592593e-03 5.92592593e-03 5.92592593e-03 5.92592593e-03\n",
      " 5.92592593e-03 5.92592593e-03 6.41975309e-03 6.41975309e-03\n",
      " 6.41975309e-03 6.91358025e-03 6.91358025e-03 7.40740741e-03\n",
      " 7.40740741e-03 7.40740741e-03 7.40740741e-03 7.90123457e-03\n",
      " 7.90123457e-03 7.90123457e-03 8.39506173e-03 8.39506173e-03\n",
      " 8.39506173e-03 8.39506173e-03 8.39506173e-03 8.88888889e-03\n",
      " 8.88888889e-03 8.88888889e-03 9.38271605e-03 9.38271605e-03\n",
      " 9.38271605e-03 9.87654321e-03 9.87654321e-03 1.03703704e-02\n",
      " 1.03703704e-02 1.18518519e-02 1.18518519e-02 1.23456790e-02\n",
      " 1.23456790e-02 1.23456790e-02 1.38271605e-02 1.38271605e-02\n",
      " 1.38271605e-02 1.38271605e-02 1.38271605e-02 1.43209877e-02\n",
      " 1.43209877e-02 1.48148148e-02 1.48148148e-02 1.48148148e-02\n",
      " 1.58024691e-02 1.58024691e-02 1.62962963e-02 1.62962963e-02\n",
      " 1.67901235e-02 1.67901235e-02 1.72839506e-02 1.72839506e-02\n",
      " 1.72839506e-02 1.87654321e-02 1.87654321e-02 1.92592593e-02\n",
      " 1.92592593e-02 2.02469136e-02 2.02469136e-02 2.02469136e-02\n",
      " 2.07407407e-02 2.07407407e-02 2.12345679e-02 2.12345679e-02\n",
      " 2.12345679e-02 2.12345679e-02 2.12345679e-02 2.22222222e-02\n",
      " 2.22222222e-02 2.61728395e-02 2.61728395e-02 2.66666667e-02\n",
      " 2.66666667e-02 3.20987654e-02 3.20987654e-02 3.45679012e-02\n",
      " 3.45679012e-02 3.60493827e-02 3.60493827e-02 3.90123457e-02\n",
      " 3.90123457e-02 4.93827160e-02 4.93827160e-02 5.23456790e-02\n",
      " 5.23456790e-02 5.38271605e-02 5.38271605e-02 5.43209877e-02\n",
      " 5.43209877e-02 5.58024691e-02 5.58024691e-02 5.97530864e-02\n",
      " 5.97530864e-02 6.61728395e-02 6.61728395e-02 7.25925926e-02\n",
      " 7.25925926e-02 7.50617284e-02 7.50617284e-02 7.65432099e-02\n",
      " 7.65432099e-02 7.75308642e-02 7.75308642e-02 9.23456790e-02\n",
      " 9.33333333e-02 9.38271605e-02 9.48148148e-02 1.07160494e-01\n",
      " 1.07160494e-01 1.20493827e-01 1.21481481e-01 1.23456790e-01\n",
      " 1.23456790e-01 1.40246914e-01 1.41234568e-01 1.45679012e-01\n",
      " 1.45679012e-01 1.66419753e-01 1.66419753e-01 1.69382716e-01\n",
      " 1.69382716e-01 1.84197531e-01 1.85185185e-01 2.04938272e-01\n",
      " 2.04938272e-01 2.29629630e-01 2.30617284e-01 2.77530864e-01\n",
      " 2.78518519e-01 2.80987654e-01 2.80987654e-01 3.43703704e-01\n",
      " 3.43703704e-01 4.11358025e-01 4.12345679e-01 5.61975309e-01\n",
      " 5.62962963e-01 5.95061728e-01 5.96049383e-01 7.60493827e-01\n",
      " 7.61481481e-01 7.91111111e-01 7.92098765e-01 7.94567901e-01\n",
      " 7.95555556e-01 8.11358025e-01 8.12345679e-01 8.32098765e-01\n",
      " 8.33086420e-01 8.89382716e-01 8.90370370e-01 9.26419753e-01\n",
      " 9.27407407e-01 9.46172840e-01 9.47160494e-01 1.00000000e+00]\n",
      "GAR\n",
      "[0.         0.53135802 0.5382716  0.54271605 0.55802469 0.56444444\n",
      " 0.5762963  0.58074074 0.58864198 0.58962963 0.59950617 0.6\n",
      " 0.60345679 0.6054321  0.61333333 0.61925926 0.62320988 0.62617284\n",
      " 0.62716049 0.63308642 0.63358025 0.63506173 0.63555556 0.63703704\n",
      " 0.63802469 0.64197531 0.64345679 0.64395062 0.6454321  0.64790123\n",
      " 0.65135802 0.65234568 0.65382716 0.65432099 0.65530864 0.65728395\n",
      " 0.6582716  0.65975309 0.66074074 0.66123457 0.66419753 0.66469136\n",
      " 0.66666667 0.66765432 0.67012346 0.67209877 0.67259259 0.67358025\n",
      " 0.67555556 0.67604938 0.67753086 0.67802469 0.68       0.68049383\n",
      " 0.68246914 0.68296296 0.68444444 0.68790123 0.68888889 0.6908642\n",
      " 0.69283951 0.69432099 0.69530864 0.69777778 0.69876543 0.69975309\n",
      " 0.70074074 0.70271605 0.70666667 0.70765432 0.70814815 0.71012346\n",
      " 0.71209877 0.71506173 0.71604938 0.71802469 0.71950617 0.72098765\n",
      " 0.72395062 0.7254321  0.72839506 0.73037037 0.73135802 0.73185185\n",
      " 0.73333333 0.73530864 0.73679012 0.73777778 0.73876543 0.74074074\n",
      " 0.74222222 0.74518519 0.74567901 0.74666667 0.74716049 0.74814815\n",
      " 0.74962963 0.75160494 0.75209877 0.75604938 0.75604938 0.75654321\n",
      " 0.75753086 0.75802469 0.75901235 0.76098765 0.76246914 0.76296296\n",
      " 0.76592593 0.76740741 0.76938272 0.77135802 0.77185185 0.77283951\n",
      " 0.77432099 0.7762963  0.77679012 0.77777778 0.77876543 0.78024691\n",
      " 0.78320988 0.78419753 0.78617284 0.78814815 0.78962963 0.79209877\n",
      " 0.79358025 0.79506173 0.79604938 0.79802469 0.79950617 0.80049383\n",
      " 0.80296296 0.80444444 0.80493827 0.80592593 0.80740741 0.81037037\n",
      " 0.81185185 0.81283951 0.81382716 0.81481481 0.81530864 0.8162963\n",
      " 0.81777778 0.81777778 0.82024691 0.82320988 0.82469136 0.82617284\n",
      " 0.82716049 0.82765432 0.8291358  0.83111111 0.8345679  0.83654321\n",
      " 0.83703704 0.83802469 0.84049383 0.84246914 0.84395062 0.8454321\n",
      " 0.84641975 0.84740741 0.84938272 0.84987654 0.85135802 0.85185185\n",
      " 0.85382716 0.85432099 0.85530864 0.85580247 0.85728395 0.85975309\n",
      " 0.86074074 0.8617284  0.86271605 0.86320988 0.86469136 0.86567901\n",
      " 0.86666667 0.86814815 0.87012346 0.87160494 0.87308642 0.87358025\n",
      " 0.8745679  0.87506173 0.87604938 0.87901235 0.88       0.88049383\n",
      " 0.88148148 0.88246914 0.88345679 0.88345679 0.88395062 0.88592593\n",
      " 0.88888889 0.89037037 0.89135802 0.89283951 0.89382716 0.89530864\n",
      " 0.8962963  0.89876543 0.90074074 0.90320988 0.90518519 0.90864198\n",
      " 0.91012346 0.91111111 0.91209877 0.91259259 0.91358025 0.91604938\n",
      " 0.91703704 0.91753086 0.91753086 0.91802469 0.91802469 0.91901235\n",
      " 0.92       0.92049383 0.92246914 0.92345679 0.92395062 0.92493827\n",
      " 0.9254321  0.92740741 0.93037037 0.9308642  0.9308642  0.93283951\n",
      " 0.93283951 0.93382716 0.93382716 0.93481481 0.93580247 0.9362963\n",
      " 0.9362963  0.93679012 0.9382716  0.93876543 0.93975309 0.94024691\n",
      " 0.94024691 0.94123457 0.94271605 0.94271605 0.94419753 0.94419753\n",
      " 0.94567901 0.94666667 0.94814815 0.94814815 0.9491358  0.95111111\n",
      " 0.95111111 0.95209877 0.95259259 0.95407407 0.95506173 0.95506173\n",
      " 0.95604938 0.95654321 0.95654321 0.95851852 0.95901235 0.95901235\n",
      " 0.95950617 0.95950617 0.96       0.96       0.96098765 0.96098765\n",
      " 0.96197531 0.96246914 0.96246914 0.96345679 0.9654321  0.96641975\n",
      " 0.96691358 0.96691358 0.96839506 0.96839506 0.96938272 0.97037037\n",
      " 0.97037037 0.97135802 0.97135802 0.97283951 0.97283951 0.97333333\n",
      " 0.97333333 0.97432099 0.97530864 0.97530864 0.97580247 0.97580247\n",
      " 0.9762963  0.9762963  0.97728395 0.9782716  0.9782716  0.97925926\n",
      " 0.97925926 0.98024691 0.98074074 0.9817284  0.98222222 0.98222222\n",
      " 0.98271605 0.98271605 0.98320988 0.98320988 0.98419753 0.98419753\n",
      " 0.98518519 0.98518519 0.98567901 0.98567901 0.98617284 0.98617284\n",
      " 0.98716049 0.98716049 0.98765432 0.98765432 0.98814815 0.98814815\n",
      " 0.9891358  0.9891358  0.98962963 0.98962963 0.99012346 0.99012346\n",
      " 0.99111111 0.99111111 0.99160494 0.99160494 0.99209877 0.99209877\n",
      " 0.99259259 0.99259259 0.99358025 0.99358025 0.99407407 0.99407407\n",
      " 0.99407407 0.99407407 0.99407407 0.99407407 0.99506173 0.99506173\n",
      " 0.99506173 0.99506173 0.99555556 0.99555556 0.99555556 0.99555556\n",
      " 0.99604938 0.99604938 0.99654321 0.99654321 0.99753086 0.99753086\n",
      " 0.99753086 0.99753086 0.99901235 0.99901235 0.99901235 0.99901235\n",
      " 0.99901235 0.99901235 0.99950617 0.99950617 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020740740740740733, 0.28377038, 0.02074074074074074, 0.020740740740740726)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_eer(y_pred_tst,labelTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
